{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fenosoa Randrianjatovo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T3P6jcegTz6A"
   },
   "source": [
    "In this second part of the lab, we will implement a language identifier trained on the same data, but using Logistic Regression instead of Naive Bayes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "fowJGMqjTz6B"
   },
   "outputs": [],
   "source": [
    "import io, sys, math\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6HSLDCkNTz6C"
   },
   "source": [
    "This function is used to build the dictionary, or vocabulary, which is a mapping from strings (or words) to integers (or indices). This will allow to build vector representations of documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "2GxcXpz7Tz6C"
   },
   "outputs": [],
   "source": [
    "def build_dict(filename, threshold=1):\n",
    "    fin = io.open(filename, 'r', encoding='utf-8')\n",
    "    word_dict, label_dict = {}, {}\n",
    "    counts = defaultdict(lambda: 0)\n",
    "    for line in fin:\n",
    "        tokens = line.split()\n",
    "        label = tokens[0]\n",
    "\n",
    "        if not label in label_dict:\n",
    "            label_dict[label] = len(label_dict)\n",
    "\n",
    "        for w in tokens[1:]:\n",
    "            counts[w] += 1\n",
    "            \n",
    "    for k, v in counts.items():\n",
    "        if v > threshold:\n",
    "            word_dict[k] = len(word_dict)\n",
    "    return word_dict, label_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z29ePna-Tz6C"
   },
   "source": [
    "This function is used to load the training dataset, and build vector representations of the training examples. In particular, a document or sentence is represented as a bag of words. Each example correspond to a sparse vector ` x` of dimension `V`, where `V` is the size of the vocabulary. The element `j` of the vector `x` is the number of times the word `j` appears in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "A6WhIcFZTz6C"
   },
   "outputs": [],
   "source": [
    "def load_data(filename, word_dict, label_dict):\n",
    "    fin = io.open(filename, 'r', encoding='utf-8')\n",
    "    data = []\n",
    "    dim = len(word_dict)\n",
    "    for line in fin:\n",
    "        tokens = line.split()\n",
    "        label = tokens[0]\n",
    "\n",
    "        yi = label_dict[label]\n",
    "        xi = np.zeros(dim)\n",
    "        for word in tokens[1:]:\n",
    "            if word in word_dict:\n",
    "                wid = word_dict[word]\n",
    "                xi[wid] += 1.0\n",
    "        data.append((yi, xi))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fsCZ7cX1Tz6C"
   },
   "source": [
    "First, let's implement the softmax function. Don't forget numerical stability!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "lpUuEJIJTz6D"
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    ### FILL CODE\n",
    "    MAX=x.max()\n",
    "    y = np.exp(x - MAX)\n",
    "    return y / np.sum(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "60CVPbtNTz6D"
   },
   "source": [
    "Now, let's implement the main training loop, by using stochastic gradient descent. The function will iterate over the examples of the training set. For each example, we will first compute the loss, before computing the gradient and performing the update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      "\n",
      "\n",
      "0 [1. 1. 1. ... 0. 0. 0.]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "word_dict, label_dict = build_dict(\"./data/train1.txt\")\n",
    "train_data = load_data(\"./data/train1.txt\", word_dict, label_dict)\n",
    "valid_data = load_data(\"./data/valid1.txt\", word_dict, label_dict)\n",
    "\n",
    "nlabels = len(label_dict)\n",
    "dim = len(word_dict)\n",
    "w = np.zeros([nlabels, dim])\n",
    "\n",
    "for Y,X in train_data:\n",
    "    print(softmax(w@X))\n",
    "    print(\"\\n\")\n",
    "    print(Y,X)\n",
    "    print(\"\\n\")\n",
    "    break \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 5826)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = np.zeros([nlabels, dim])\n",
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "id": "zU_IJUPiTz6D"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████                              | 1/2 [00:02<00:02,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 \t train Loss: -712.612723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████| 2/2 [00:04<00:00,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 \t train Loss: -690.856818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def sgd(w, data, niter):\n",
    "    nlabels, dim = w.shape\n",
    "#     return w.shape[1]\n",
    "    for iter in tqdm(range(niter)):\n",
    "        ### FILL CODE\n",
    "        lr=0.5\n",
    "        ema_loss=None\n",
    "        training_loss = 0.00\n",
    "        for y, x in data:\n",
    "\n",
    "            pred = softmax(w@x)\n",
    "            training_loss += np.log(pred[y])\n",
    "            \n",
    "            target_one = np.zeros_like(pred)\n",
    "            target_one[y] = 1.0\n",
    "            \n",
    "            grad = (target_one - pred).reshape((nlabels, -1)) * x.reshape((-1, dim))\n",
    "            \n",
    "            w+=lr * grad\n",
    "            if ema_loss is None:\n",
    "                ema_loss = training_loss\n",
    "            else:\n",
    "                ema_loss += (training_loss - ema_loss) * 0.01\n",
    "\n",
    "        # Print out progress the end of epoch.\n",
    "        print(\"Train Epoch: {} \\t train Loss: {:.6f}\".format(iter+1, ema_loss))\n",
    "     \n",
    "    return w\n",
    "\n",
    "\n",
    "if \"__main__\"==__name__ :\n",
    "    sgd(w, train_data, 2)\n",
    "#     print(w)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqp734Z1Tz6D"
   },
   "source": [
    "The next function will predict the most probable label corresponding to example `x`, given the trained classifier `w`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "id": "cD00Y3KyTz6D"
   },
   "outputs": [],
   "source": [
    "def predict(w, x):\n",
    "    ## FILL CODE\n",
    "    pred= softmax(w@x)\n",
    "    return np.argmax(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Zhx3TChTz6D"
   },
   "source": [
    "Finally, this function will compute the accuracy of a trained classifier `w` on a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "id": "6-H9UVFsTz6D"
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(w, valid_data):\n",
    "    ## FILL CODE\n",
    "    accuracy = 0.0\n",
    "    for y, x in valid_data:\n",
    "        y_pred = predict(w, x)\n",
    "        if y == y_pred :\n",
    "            accuracy += 1.0\n",
    "    return ( accuracy / len(valid_data))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "id": "Njk6PkjOTz6E"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "** Logistic Regression **\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████▉                                                     | 1/10 [00:01<00:17,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 \t train Loss: -4877.992477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|███████████▊                                               | 2/10 [00:04<00:16,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 \t train Loss: -2034.937700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|█████████████████▋                                         | 3/10 [00:06<00:14,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 \t train Loss: -1544.218800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|███████████████████████▌                                   | 4/10 [00:08<00:12,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 \t train Loss: -1302.662100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████████████▌                             | 5/10 [00:10<00:11,  2.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 \t train Loss: -1157.190879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|███████████████████████████████████▍                       | 6/10 [00:13<00:08,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 \t train Loss: -1060.389978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|█████████████████████████████████████████▎                 | 7/10 [00:15<00:06,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 \t train Loss: -992.017649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|███████████████████████████████████████████████▏           | 8/10 [00:17<00:04,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8 \t train Loss: -941.585208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████████████████████████████████████████████████      | 9/10 [00:19<00:02,  2.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 \t train Loss: -903.073523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 10/10 [00:21<00:00,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 10 \t train Loss: -872.807390\n",
      "\n",
      "Validation accuracy: 93.30%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\")\n",
    "print(\"** Logistic Regression **\")\n",
    "print(\"\")\n",
    "\n",
    "word_dict, label_dict = build_dict(\"./data/train1.txt\")\n",
    "train_data = load_data(\"./data/train1.txt\", word_dict, label_dict)\n",
    "valid_data = load_data(\"./data/valid1.txt\", word_dict, label_dict)\n",
    "\n",
    "nlabels = len(label_dict)\n",
    "dim = len(word_dict)\n",
    "w = np.zeros([nlabels, dim])\n",
    "w = sgd(w, train_data, 10)\n",
    "print(\"\")\n",
    "print(\"Validation accuracy: %.2f%s\" %(compute_accuracy(w, valid_data), \"%\"))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "** Logistic Regression **\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████▉                                                     | 1/10 [00:24<03:36, 24.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 \t train Loss: -24139.380184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|███████████▊                                               | 2/10 [00:49<03:19, 24.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 \t train Loss: -15586.643506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|█████████████████▋                                         | 3/10 [01:14<02:53, 24.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 \t train Loss: -14219.622659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|███████████████████████▌                                   | 4/10 [01:38<02:26, 24.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 \t train Loss: -13555.419423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████████████▌                             | 5/10 [02:05<02:07, 25.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 \t train Loss: -13157.904336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|███████████████████████████████████▍                       | 6/10 [02:30<01:40, 25.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 \t train Loss: -12895.436810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|█████████████████████████████████████████▎                 | 7/10 [02:59<01:19, 26.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 \t train Loss: -12711.779095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|███████████████████████████████████████████████▏           | 8/10 [03:29<00:55, 27.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8 \t train Loss: -12576.269581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|█████████████████████████████████████████████████████      | 9/10 [03:57<00:27, 27.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 \t train Loss: -12472.919024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 10/10 [04:23<00:00, 26.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 10 \t train Loss: -12391.810220\n",
      "\n",
      "Validation accuracy: 93.90%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\")\n",
    "print(\"** Logistic Regression **\")\n",
    "print(\"\")\n",
    "\n",
    "word_dict, label_dict = build_dict(\"./data/train1.txt\")\n",
    "train_data = load_data(\"./data/train2.txt\", word_dict, label_dict)\n",
    "valid_data = load_data(\"./data/valid2.txt\", word_dict, label_dict)\n",
    "\n",
    "nlabels = len(label_dict)\n",
    "dim = len(word_dict)\n",
    "w = np.zeros([nlabels, dim])\n",
    "w = sgd(w, train_data, 10)\n",
    "print(\"\")\n",
    "print(\"Validation accuracy: %.2f%s\" %(compute_accuracy(w, valid_data), \"%\"))\n",
    "print(\"\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copy of logistic_regression.ipynb",
   "provenance": [
    {
     "file_id": "1JjbXIdABu8YunuxBmC96JTqfu9_vWDF7",
     "timestamp": 1652191881091
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
